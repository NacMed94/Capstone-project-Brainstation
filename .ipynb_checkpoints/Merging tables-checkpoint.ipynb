{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a090e2ca",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Initial-data-exploration-and-cleaning\" data-toc-modified-id=\"Initial-data-exploration-and-cleaning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Initial data exploration and cleaning</a></span></li><li><span><a href=\"#Extracting-past-care-units-features\" data-toc-modified-id=\"Extracting-past-care-units-features-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Extracting past care units features</a></span></li><li><span><a href=\"#Joining-with-ATC-level-1-prescriptions-features\" data-toc-modified-id=\"Joining-with-ATC-level-1-prescriptions-features-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Joining with ATC level 1 prescriptions features</a></span></li><li><span><a href=\"#Merging-split-rows\" data-toc-modified-id=\"Merging-split-rows-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Merging split rows</a></span><ul class=\"toc-item\"><li><span><a href=\"#Invalid-discharges\" data-toc-modified-id=\"Invalid-discharges-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Invalid discharges</a></span></li><li><span><a href=\"#Merging-short-transfers\" data-toc-modified-id=\"Merging-short-transfers-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Merging short transfers</a></span></li><li><span><a href=\"#Invalid-transfers\" data-toc-modified-id=\"Invalid-transfers-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Invalid transfers</a></span></li></ul></li><li><span><a href=\"#Demographic-features\" data-toc-modified-id=\"Demographic-features-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Demographic features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Patients-table\" data-toc-modified-id=\"Patients-table-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Patients table</a></span></li><li><span><a href=\"#Admissions\" data-toc-modified-id=\"Admissions-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Admissions</a></span></li></ul></li><li><span><a href=\"#Joining-with-ATC-level-2-prescriptions-features\" data-toc-modified-id=\"Joining-with-ATC-level-2-prescriptions-features-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Joining with ATC level 2 prescriptions features</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c521c",
   "metadata": {},
   "source": [
    "In this notebook, we clean and extract features for the [transfers](https://mimic.mit.edu/docs/iv/modules/hosp/transfers/) table and add features from prescriptions (features extracted in other notebook), [admissions](https://mimic.mit.edu/docs/iv/modules/hosp/admissions/) and [patients](https://mimic.mit.edu/docs/iv/modules/hosp/patients/) tables. All data wrangling, cleaning and feature extraction is going to be done with one goal in mind: Predicting the hospital department/care unit a patient is going to end up next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2c3cfed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T16:10:19.826831Z",
     "start_time": "2022-11-06T16:10:19.819762Z"
    },
    "init_cell": true,
    "tags": [
     "Iniitialization"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import nachopy\n",
    "import importlib\n",
    "import pickle\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32014e4f",
   "metadata": {},
   "source": [
    "## Initial data exploration and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c0c75",
   "metadata": {},
   "source": [
    "We will begin by cleaning and the data and performing a preliminary exploratory analysis (EDA on other notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b40070e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T16:08:04.136111Z",
     "start_time": "2022-11-06T16:08:01.426467Z"
    },
    "tags": [
     "Transfers",
     "reset"
    ]
   },
   "outputs": [],
   "source": [
    "transfers = pd.read_csv('G:/My Drive/Capstone-Data/MIMIC IV/transfers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7322190",
   "metadata": {},
   "source": [
    "If we sort values by subject_id and intime we can look at all the transfers for a patient in chronological order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdfcd87f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T16:08:09.136451Z",
     "start_time": "2022-11-06T16:08:04.139112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>transfer_id</th>\n",
       "      <th>eventtype</th>\n",
       "      <th>careunit</th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853.0</td>\n",
       "      <td>33258284</td>\n",
       "      <td>ED</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>2180-05-06 19:17:00</td>\n",
       "      <td>2180-05-06 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853.0</td>\n",
       "      <td>35223874</td>\n",
       "      <td>admit</td>\n",
       "      <td>Transplant</td>\n",
       "      <td>2180-05-06 23:30:00</td>\n",
       "      <td>2180-05-07 17:21:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853.0</td>\n",
       "      <td>36904543</td>\n",
       "      <td>discharge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2180-05-07 17:21:27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22841357.0</td>\n",
       "      <td>38112554</td>\n",
       "      <td>ED</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>2180-06-26 15:54:00</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22841357.0</td>\n",
       "      <td>34703856</td>\n",
       "      <td>admit</td>\n",
       "      <td>Transplant</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "      <td>2180-06-27 18:49:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22841357.0</td>\n",
       "      <td>34100253</td>\n",
       "      <td>discharge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2180-06-27 18:49:12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034.0</td>\n",
       "      <td>32952584</td>\n",
       "      <td>ED</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>2180-07-22 16:24:00</td>\n",
       "      <td>2180-07-23 05:54:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034.0</td>\n",
       "      <td>39399961</td>\n",
       "      <td>ED</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>2180-07-23 05:54:00</td>\n",
       "      <td>2180-07-23 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034.0</td>\n",
       "      <td>39553978</td>\n",
       "      <td>admit</td>\n",
       "      <td>Medical Intensive Care Unit (MICU)</td>\n",
       "      <td>2180-07-23 14:00:00</td>\n",
       "      <td>2180-07-23 23:50:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034.0</td>\n",
       "      <td>35888873</td>\n",
       "      <td>transfer</td>\n",
       "      <td>Transplant</td>\n",
       "      <td>2180-07-23 23:50:47</td>\n",
       "      <td>2180-07-24 19:52:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject_id     hadm_id  transfer_id  eventtype  \\\n",
       "0     10000032  22595853.0     33258284         ED   \n",
       "1     10000032  22595853.0     35223874      admit   \n",
       "2     10000032  22595853.0     36904543  discharge   \n",
       "5     10000032  22841357.0     38112554         ED   \n",
       "4     10000032  22841357.0     34703856      admit   \n",
       "3     10000032  22841357.0     34100253  discharge   \n",
       "9     10000032  29079034.0     32952584         ED   \n",
       "13    10000032  29079034.0     39399961         ED   \n",
       "14    10000032  29079034.0     39553978      admit   \n",
       "10    10000032  29079034.0     35888873   transfer   \n",
       "\n",
       "                              careunit               intime  \\\n",
       "0                 Emergency Department  2180-05-06 19:17:00   \n",
       "1                           Transplant  2180-05-06 23:30:00   \n",
       "2                                  NaN  2180-05-07 17:21:27   \n",
       "5                 Emergency Department  2180-06-26 15:54:00   \n",
       "4                           Transplant  2180-06-26 21:31:00   \n",
       "3                                  NaN  2180-06-27 18:49:12   \n",
       "9                 Emergency Department  2180-07-22 16:24:00   \n",
       "13                Emergency Department  2180-07-23 05:54:00   \n",
       "14  Medical Intensive Care Unit (MICU)  2180-07-23 14:00:00   \n",
       "10                          Transplant  2180-07-23 23:50:47   \n",
       "\n",
       "                outtime  \n",
       "0   2180-05-06 23:30:00  \n",
       "1   2180-05-07 17:21:27  \n",
       "2                   NaN  \n",
       "5   2180-06-26 21:31:00  \n",
       "4   2180-06-27 18:49:12  \n",
       "3                   NaN  \n",
       "9   2180-07-23 05:54:00  \n",
       "13  2180-07-23 14:00:00  \n",
       "14  2180-07-23 23:50:47  \n",
       "10  2180-07-24 19:52:58  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfers.sort_values(by = ['subject_id','intime']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a4169",
   "metadata": {},
   "source": [
    "So each row describes an event, that can be admission, ED (admission to Emergency Department?), discharge and transfer (among other possibly). In the table above, the first three rows show the first stay of patient 100000032. They arrived in the ED, where they spent 6 h, they were then transferred to the transplants care unti and were discharged after a day. As we want to predict the destination care unit for each transfer we should probably change the organisation of this df by adding the original care unit of the next transfer as a column of the previous row. This column, which we will call \"event\" will be our class (the outcome we want to predict). This way we can delete all discharge rows which are redundant anyway. \n",
    "\n",
    "Also note that some transfers appear to be continuous (rows with indices 9 and 13) but are two separate rows for some reason. We should merge these rows at some point as these are probably mistakes (a transfer to the same care unit does not make much sense). Let's explore and clean the data a bit more first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d2b5e02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T16:08:09.871521Z",
     "start_time": "2022-11-06T16:08:09.138453Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1991704 entries, 0 to 1991703\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count    Dtype \n",
      "---  ------       --------------    ----- \n",
      " 0   subject_id   1991704 non-null  int64 \n",
      " 1   hadm_id      1644130 non-null  Int64 \n",
      " 2   transfer_id  1991704 non-null  int64 \n",
      " 3   eventtype    1991704 non-null  object\n",
      " 4   careunit     1537380 non-null  object\n",
      " 5   intime       1991704 non-null  object\n",
      " 6   outtime      1537383 non-null  object\n",
      "dtypes: Int64(1), int64(2), object(4)\n",
      "memory usage: 108.3+ MB\n"
     ]
    }
   ],
   "source": [
    "transfers['hadm_id'] = transfers.hadm_id.astype('Int64')\n",
    "transfers.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791769a1",
   "metadata": {},
   "source": [
    "We have some null values for the care unit, hadm_ids and outtimes. The hadm_id null values are problematic as these can be useful to merge information from other tables. The care unit and outtime null values are probably related to the discharge events (above we saw that discharge events have no care unit). Let's convert the datetime columns to datetime. It does not make much sense exploring these as they are as the dates are randomised to deidentify the data. The dates for each patient are consistent, though, so we can use them to extract duration for example or to sort events for a given patient. We can create this new feature now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7352c6cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T16:36:29.711749Z",
     "start_time": "2022-11-06T16:36:29.667640Z"
    },
    "tags": [
     "Transfers",
     "reset"
    ]
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Let's have these as datetime, which will be useful to compare later\n",
    "transfers['intime'] = pd.to_datetime(transfers.intime, infer_datetime_format=True)\n",
    "transfers['outtime'] = pd.to_datetime(transfers.outtime, infer_datetime_format=True)\n",
    "transfers['duration'] = (transfers.outtime - transfers.intime)/datetime.timedelta(hours = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18272fd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T16:08:12.632515Z",
     "start_time": "2022-11-06T16:08:11.683675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id      315460\n",
       "hadm_id         454324\n",
       "transfer_id    1991704\n",
       "eventtype            4\n",
       "careunit            39\n",
       "intime         1980736\n",
       "outtime        1532471\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfers.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74410233",
   "metadata": {},
   "source": [
    "Transfer id has as many unique values as the length of the df, so we will probably set this column as index later. This will come in handy to merge more tables in the future after feature expansion. We have 315460 patients and 454324 stays (with some missing values). The number of stays is higher than the number of patients because a patient may have more than one stay in the hospital (a stay can be considered a series of consecutive transfers). Let's see if we have any duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9d325d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T16:08:13.912695Z",
     "start_time": "2022-11-06T16:08:12.633525Z"
    },
    "tags": [
     "Transfers",
     "reset"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We deleted 8 duplicate rows\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicates without the transfer_id column ensures we find any duplicate rows that were assigned different transfer_ids \n",
    "transfers_dupl = transfers.drop(columns = 'transfer_id').duplicated()\n",
    "transfers = transfers[~transfers_dupl]\n",
    "print(f'We deleted {transfers_dupl.sum()} duplicate rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c1df37",
   "metadata": {},
   "source": [
    "Now let's see why we have missing values in outtime. Are these all for discharge events?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fb21ebe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T16:08:14.285564Z",
     "start_time": "2022-11-06T16:08:13.914628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject_id     190279\n",
      "hadm_id        454321\n",
      "transfer_id    454321\n",
      "eventtype           1\n",
      "careunit            0\n",
      "intime         454236\n",
      "outtime             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(transfers[transfers.outtime.isna()].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a4a985",
   "metadata": {},
   "source": [
    "No care units and only one unique eventtype, which we have seen above to be Discharge. We also know that the number of missing care units was the same as the number of missing outtimes (minus three) so practially all the missing care units are due to discharge events as well. Let's reorganise the dataframe so that we include the outcome/event in the same row as the original careunit. We can sort by subject_id and intime (inplace this time) and add a shifted series of the care unit as the \"event\" column in the df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "180156ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T16:08:15.439736Z",
     "start_time": "2022-11-06T16:08:14.287565Z"
    },
    "tags": [
     "Transfers",
     "reset"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>transfer_id</th>\n",
       "      <th>eventtype</th>\n",
       "      <th>careunit</th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1991702</th>\n",
       "      <td>19999987</td>\n",
       "      <td>23865745</td>\n",
       "      <td>36195440</td>\n",
       "      <td>admit</td>\n",
       "      <td>Trauma SICU (TSICU)</td>\n",
       "      <td>2145-11-02 22:59:00</td>\n",
       "      <td>2145-11-04 21:29:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         subject_id   hadm_id  transfer_id eventtype             careunit  \\\n",
       "1991702    19999987  23865745     36195440     admit  Trauma SICU (TSICU)   \n",
       "\n",
       "                     intime             outtime  \n",
       "1991702 2145-11-02 22:59:00 2145-11-04 21:29:30  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transfers.dropna(axis = 0, subset = 'careunit', inplace = True) # Dropping the 3 rows with missing care units we had seen before\n",
    "\n",
    "display(transfers.tail(1)) # Last row is a discharge\n",
    "transfers.sort_values(by=['subject_id','intime'],inplace = True)\n",
    "events = transfers.careunit.loc[1:].values # From second row, getting the destination careunit (class)\n",
    "transfers = transfers.iloc[:-1] # Until second-to-last row (last row is a discharge)\n",
    "transfers['event'] = events # Adding the shifted series as new column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48117e4",
   "metadata": {},
   "source": [
    "If we have done it correctly, there should be one more null value in the new event column than for the discharge events, as these care unit null values correspond with discharge events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06eb2ef1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T16:08:15.704241Z",
     "start_time": "2022-11-06T16:08:15.441672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sum(transfers.event.isna()))\n",
    "print(sum(transfers.eventtype == 'discharge'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce06f664",
   "metadata": {},
   "source": [
    "It's correct, so we can fill the NaNs in the event column as discharges and drop the (old) discharge columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd7d6d0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T16:09:14.705856Z",
     "start_time": "2022-11-06T16:09:14.606453Z"
    },
    "tags": [
     "Transfers",
     "reset"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have 0 null values in outtime\n"
     ]
    }
   ],
   "source": [
    "transfers['event'] = transfers.event.fillna('Discharge') # Filling the null values with 'Discharge' event\n",
    "transfers.reset_index(inplace = True,drop = True) # Resetting the index so it corresponds with sorted subject_id\n",
    "print(f'Now we have {transfers.outtime.isna().sum()} null values in outtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f121d2",
   "metadata": {},
   "source": [
    "## Extracting past care units features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871650d",
   "metadata": {},
   "source": [
    "We are going to be merging transfer rows and deleting others based on several criteria for data cleaning, but before, we can extract one important feature: past transfers. We do this before cleaning the transfers table because we are going to be deleting transfers that we do not want to predict (for example transfer with short durations).\n",
    "\n",
    "We are going to extract the number of times (frequency) a patient has been in a care unit as a feature as well as the total duration of those past stays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8cf9e3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T19:11:28.691246Z",
     "start_time": "2022-10-31T17:15:52.737102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19999987/19999987\n"
     ]
    }
   ],
   "source": [
    "# Initialising feature extraction df\n",
    "prev_transfers = transfers[['subject_id','transfer_id','careunit','duration']]\n",
    "\n",
    "# Creating new dummy variables from care units\n",
    "prev_transfers = pd.concat([prev_transfers.drop(columns ='careunit'),\\\n",
    "                            pd.get_dummies(prev_transfers.careunit,prefix = 'freq'),\n",
    "                            pd.get_dummies(prev_transfers.careunit,prefix = 'dur')],axis = 1)\n",
    "\n",
    "# Now they are identical sets of dummy variables encoding the column \"careunit\", but we will fill these with past care units\n",
    "\n",
    "# We can set the index as the subject_id as we will do this patient by patient\n",
    "prev_transfers.set_index('subject_id',inplace = True)\n",
    "dur_cols = prev_transfers.columns[41:] # Columns indexes for duration features\n",
    "\n",
    "# Duration features are now equal to the duration of the stay in the current transfer (for the origin care unit)\n",
    "prev_transfers[dur_cols] = prev_transfers[dur_cols].multiply(prev_transfers.duration.values,axis = 0).\n",
    "prev_transfers.drop(columns = 'duration',inplace = True) # We drop duration as we don't need it anymore\n",
    "\n",
    "cols = prev_transfers.columns[1:] # Dummy columns (to be edited)\n",
    "# Iterating over each subject\n",
    "subjects = prev_transfers.index.unique()\n",
    "for subject in subjects:\n",
    "    # Getting all rows for same subject\n",
    "    summing_df = prev_transfers.loc[subject,cols]\n",
    "    # Iterating over rows from the last (most recent) to the first (oldest)\n",
    "    summing_len = len(summing_df)\n",
    "    for i in range(summing_len-1,-1,-1):\n",
    "        # Summing durations and frequencies \n",
    "        summing_df.iloc[i] = summing_df.iloc[:i].sum(axis = 0)\n",
    "    # Features overwritten after summation\n",
    "    prev_transfers.loc[subject,cols] = summing_df  \n",
    "    print(f'{subject}/19999987', end = '\\r') # Just a counter to check progress (really long script)\n",
    "\n",
    "print()\n",
    "# Setting index as transfer_id (and dropping the subject_id index) and saving pickle\n",
    "prev_transfers.set_index('transfer_id', drop = True, inplace = True)\n",
    "prev_transfers.to_pickle('G:/My Drive/Capstone Data/pickles/prev_transfers.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9edc2f",
   "metadata": {},
   "source": [
    "## Joining with ATC level 1 prescriptions features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ae0915",
   "metadata": {},
   "source": [
    "As we are going to be merging rows, we can merge with prescriptions beforehand. This way, during the merging, we can just add the prescriptions of the two rows. We are creating a new feature on prescriptions, pres_number. This is just a counter of all the prescriptions in a given transfer. We will also be setting any remaining null values (right now -1) as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49407c1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T16:58:27.662197Z",
     "start_time": "2022-11-06T16:57:53.244913Z"
    }
   },
   "outputs": [],
   "source": [
    "prescriptions = pd.read_pickle('G:/My Drive/Capstone-Data/pickles/prescriptions_v1.pkl')\n",
    "prescriptions.drop(columns = ['drug_type','drug','formulary_drug_cd','gsn','ndc'],inplace = True)\n",
    "prescriptions[prescriptions.A<0] = 0 # Setting all null values to 0\n",
    "\n",
    "\n",
    "# Initialising prescription number feature as 1\n",
    "prescriptions['pres_number'] = 1\n",
    "cat = list(prescriptions.drop(columns = ['subject_id','hadm_id','starttime']).columns)\n",
    "cat.append('pres_number') # appending to categorical columns list for later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce949a53",
   "metadata": {},
   "source": [
    "To merge the prescriptions categories to transfers we will have to use subject_id (missing values on hadm_id) and then remove all the rows for prescriptions whose start time is not between the intime and the outtime of the transfer. This will be done in a loop by chunks to avoid running out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c78f4fb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T06:22:38.601998Z",
     "start_time": "2022-11-01T06:15:36.181587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop 15/15\n"
     ]
    }
   ],
   "source": [
    "# Initialising variables for loop\n",
    "# Getting only necessary columns for the row selection\n",
    "dfl = transfers[['subject_id','intime','outtime','transfer_id']].set_index('subject_id').sort_index()\n",
    "# Adding hadm_id because it will be useful to fill hadm_id column on transfers\n",
    "dfr = prescriptions.drop(columns = 'hadm_id').set_index('subject_id').sort_index() \n",
    "\n",
    "n = 15\n",
    "nums = copy.deepcopy(cat)\n",
    "non_nums = ['subject_id','intime','outtime','starttime']\n",
    "\n",
    "df_merged = pd.DataFrame(columns = nums)\n",
    "\n",
    "len_r = len(dfr)\n",
    "max_l = dfl.index.max()\n",
    "max_r = dfr.index.max()\n",
    "\n",
    "inc = round(len(dfr)/n)\n",
    "ind_r = copy.copy(inc)\n",
    "sub_1 = 0\n",
    "sub_2 = 0\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    ind_r += inc \n",
    "    if ind_r < len_r:\n",
    "        sub_2 = dfr.iloc[ind_r].name # Getting last index in increment \n",
    "    else:\n",
    "        sub_2 = max_l # Getting last subject on left (transfers) df when reaching the end of the df\n",
    "    \n",
    "    dfr_chunk = dfr.loc[sub_1:sub_2]\n",
    "    dfl_chunk = dfl.loc[sub_1:sub_2]\n",
    "    # Inner join as any rows without prescriptions are lost anyway (will be recovered later)\n",
    "    df_merging = pd.merge(dfl_chunk,dfr_chunk, on = 'subject_id') \n",
    "\n",
    "    # Selecting only rows where the prescription is between starttime and outtime \n",
    "    valid_rows = (df_merging.starttime>=df_merging.intime) & (df_merging.starttime<df_merging.outtime)\n",
    "    \n",
    "    df_merging = df_merging[valid_rows].reset_index(drop = False) # Putting subject_id away from index and back in columns\n",
    "    # Retrieving hadm_id from prescriptions to merge later\n",
    "    df_merging = df_merging.groupby('transfer_id')[nums].sum() # We sum to frequency encode prescriptions\n",
    "    # Building merged dataframe\n",
    "    df_merged = pd.concat([df_merged, df_merging],axis = 0)\n",
    "    \n",
    "    sub_1 = sub_2 + 1\n",
    "    \n",
    "    print(f'Loop {i + 1}/{n}',end='\\r')\n",
    "print()\n",
    "\n",
    "# Setting index as transfer id and left joining df_merged\n",
    "transfers.set_index('transfer_id',inplace = True)\n",
    "transfers = transfers.join(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f197832",
   "metadata": {},
   "source": [
    "Now we will have floats in the categories, let's see which data type is appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2fe5bde8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T16:58:37.571340Z",
     "start_time": "2022-11-06T16:58:37.481239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value in categorical variables is 1109\n"
     ]
    }
   ],
   "source": [
    "print(f'Maximum value in categorical variables is {transfers2[cat].max().max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912af474",
   "metadata": {},
   "source": [
    "Given that the maximum is 1109 (surprisingly high), we can set the data type as uint16 (int16 would be enough but same memory as int16 and we do not have negative values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d8559f9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T06:54:34.910162Z",
     "start_time": "2022-11-01T06:54:34.514145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1537372 entries, 0 to 1537371\n",
      "Data columns (total 24 columns):\n",
      " #   Column       Non-Null Count    Dtype         \n",
      "---  ------       --------------    -----         \n",
      " 0   transfer_id  1537372 non-null  int64         \n",
      " 1   subject_id   1537372 non-null  int64         \n",
      " 2   hadm_id      1189806 non-null  Int64         \n",
      " 3   eventtype    1537372 non-null  object        \n",
      " 4   careunit     1537372 non-null  object        \n",
      " 5   intime       1537372 non-null  datetime64[ns]\n",
      " 6   outtime      1537372 non-null  datetime64[ns]\n",
      " 7   event        1537372 non-null  object        \n",
      " 8   A            1537372 non-null  uint16        \n",
      " 9   B            1537372 non-null  uint16        \n",
      " 10  C            1537372 non-null  uint16        \n",
      " 11  D            1537372 non-null  uint16        \n",
      " 12  G            1537372 non-null  uint16        \n",
      " 13  H            1537372 non-null  uint16        \n",
      " 14  J            1537372 non-null  uint16        \n",
      " 15  L            1537372 non-null  uint16        \n",
      " 16  M            1537372 non-null  uint16        \n",
      " 17  N            1537372 non-null  uint16        \n",
      " 18  P            1537372 non-null  uint16        \n",
      " 19  R            1537372 non-null  uint16        \n",
      " 20  S            1537372 non-null  uint16        \n",
      " 21  V            1537372 non-null  uint16        \n",
      " 22  pres_number  1537372 non-null  uint16        \n",
      " 23  duration     1537372 non-null  float64       \n",
      "dtypes: Int64(1), datetime64[ns](2), float64(1), int64(2), object(3), uint16(15)\n",
      "memory usage: 151.0+ MB\n"
     ]
    }
   ],
   "source": [
    "transfers[cat] = transfers[cat].fillna(0) # Filling all missing values assuming that these transfers did not receive prescriptions\n",
    "transfers[cat] = transfers[cat].astype('uint16')\n",
    "transfers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a068227",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T09:52:57.828122Z",
     "start_time": "2022-11-01T09:52:57.626485Z"
    }
   },
   "outputs": [],
   "source": [
    "transfers.to_pickle('G:/My Drive/Capstone-Data/pickles/dirty_transpres.pkl') # Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf295bb",
   "metadata": {},
   "source": [
    "## Merging split rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88980f84",
   "metadata": {},
   "source": [
    "Next step is merging transfers that are invalid (discharge followed immediately by another transfer), those that are the same event (event = careunit) or that are very close together. We are also merging short transfers, as these will be scheduled at least a few hours in advance. We can simplify our dataset and improve our model by merging these rows when possible. To be conservative, we can set this lower transfer duration threshold at 3 h."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d58569",
   "metadata": {},
   "source": [
    "### Invalid discharges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f0d490",
   "metadata": {},
   "source": [
    "We will first merge any discharge rows followed by a close transfer. We can do that by comparing and substituting values of shifted dataframe and series, as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c13b2d2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T17:24:38.688016Z",
     "start_time": "2022-11-06T17:24:33.618508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22932 discharge rows followed by another row.\n",
      "Number of loops: 3\n",
      "Rows merged: 23025\n"
     ]
    }
   ],
   "source": [
    "transfers.sort_values(by = ['subject_id','intime'],inplace = True) # Just in case\n",
    "\n",
    "# Creating a boolean series to later select rows where discharge was closely followed by another transfer for same patient\n",
    "disch_events = transfers.event.str.contains('Discharge').values & \\\n",
    "               (transfers.subject_id.shift(-1) == transfers.subject_id).values & \\\n",
    "               (transfers.outtime > transfers.intime.shift(-1) - np.timedelta64(3,'h')).values\n",
    "\n",
    "print(f'There are {sum(disch_events)} discharge rows followed by another row.')\n",
    "\n",
    "# Storing original length to know how many rows we remove in total\n",
    "original_len = len(transfers)\n",
    "transfers_len = np.inf # At least one loop\n",
    "i = 0\n",
    "\n",
    "# While there is a decrease in the number of rows\n",
    "while len(transfers) < transfers_len:\n",
    "    transfers_len = len(transfers)    \n",
    "    \n",
    "    # Same bool as before\n",
    "    disch_events = transfers.event.str.contains('Discharge').values & \\\n",
    "               (transfers.subject_id.shift(-1) == transfers.subject_id).values & \\\n",
    "               (transfers.outtime > transfers.intime.shift(-1) - np.timedelta64(3,'h')).values\n",
    "    \n",
    "    # Bool for the following rows to these events\n",
    "    disch_next = np.roll(disch_events,1)\n",
    "    \n",
    "    transfers.loc[disch_events,'outtime'] = transfers.outtime[disch_next].values # Outtime equal to that of next row\n",
    "    transfers.loc[disch_events,'event'] = transfers.event[disch_next].values # Event equal to that of next row\n",
    "    transfers.loc[disch_events,cat] += transfers.loc[disch_next,cat].values # Cats added for both rows\n",
    "    \n",
    "    # Removing the second row of each merged row (info now on first)\n",
    "    transfers = transfers[~disch_next]\n",
    "    \n",
    "    i +=1\n",
    "    \n",
    "print('Number of loops:',i)\n",
    "print('Rows merged:', original_len - len(transfers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9804033",
   "metadata": {},
   "source": [
    "We have removed 30k rows. After merging we kept applying the algorithm until there were no more disch_events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef527e01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T06:58:16.243412Z",
     "start_time": "2022-11-01T06:58:15.923943Z"
    }
   },
   "source": [
    "### Merging short transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf4f833",
   "metadata": {},
   "source": [
    "Now we will update duration and merge any rows with duration shorter than 3h. As introduced above, predicting such transfer will be difficult and possibly pointless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "846d60a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T17:24:57.133353Z",
     "start_time": "2022-11-06T17:24:56.067611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loops: 3\n",
      "Rows merged: 16425\n"
     ]
    }
   ],
   "source": [
    "transfers['duration'] = (transfers.outtime - transfers.intime)/np.timedelta64(1,'h')\n",
    "\n",
    "original_len = len(transfers)\n",
    "transfers_len = np.inf # At least one loop\n",
    "\n",
    "i = 0\n",
    "\n",
    "while len(transfers) < transfers_len:\n",
    "    transfers_len = len(transfers)    \n",
    "    # We first create a new column called duration, which can serve as a new feature\n",
    "    # We are interested in rows with duration under 3 hours...\n",
    "    short_transfers = (transfers['duration'] < 3).values\n",
    "    # And whose preceding row has the same subject_id\n",
    "    short_transfers *= (transfers.subject_id.shift() == transfers.subject_id).values\n",
    "    # And where the intime is close to the outtime of the preceding row (within 3 h)\n",
    "    short_transfers *= (transfers.intime < transfers.outtime.shift() + np.timedelta64(3,'h')).values\n",
    "    # We can edit the intime now\n",
    "    short_prev = np.roll(short_transfers,-1)\n",
    "    \n",
    "    transfers.loc[short_transfers,'intime'] = transfers.intime[short_prev].values\n",
    "    transfers.loc[short_transfers,'careunit'] = transfers.careunit[short_prev].values\n",
    "    transfers.loc[short_transfers,'eventtype'] = transfers.eventtype[short_prev].values\n",
    "    transfers.loc[short_transfers,cat] += transfers.loc[short_prev,cat].values\n",
    "    \n",
    "    transfers.reset_index(drop = True, inplace = True)\n",
    "    transfers['duration'] = (transfers.outtime - transfers.intime)/np.timedelta64(1,'h')\n",
    "    \n",
    "    transfers = transfers[~short_prev]\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "print('Number of loops:',i)\n",
    "print('Rows merged:', original_len - len(transfers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106a9b5",
   "metadata": {},
   "source": [
    "Now we can do the same but on following rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ebdbbc2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T17:25:01.234656Z",
     "start_time": "2022-11-06T17:25:00.393224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loops 2\n",
      "Rows merged: 36988\n"
     ]
    }
   ],
   "source": [
    "transfers_len = np.inf # At least one loop\n",
    "i = 0\n",
    "original_len = len(transfers)\n",
    "\n",
    "while len(transfers) < transfers_len:\n",
    "    transfers_len = len(transfers)    \n",
    "    # We are interested in rows with duration under 3 hours...\n",
    "    short_transfers = (transfers['duration'] < 3).values\n",
    "    # And whose preceding row has the same subject_id\n",
    "    short_transfers *= (transfers.subject_id == transfers.subject_id.shift(-1)).values\n",
    "    # And where the intime is close to the outtime of the preceding row (within 3 h)\n",
    "    short_transfers *= (transfers.outtime > transfers.intime.shift(-1) - np.timedelta64(3,'h')).values\n",
    "    # # We can edit the intime now\n",
    "    \n",
    "    short_next = np.roll(short_transfers,1)\n",
    "    transfers.loc[short_transfers,'outtime'] = transfers.outtime[short_next].values\n",
    "    transfers.loc[short_transfers,'event'] = transfers.event[short_next].values\n",
    "    transfers.loc[short_transfers,cat] += transfers.loc[short_next,cat].values\n",
    "    \n",
    "    transfers = transfers[~short_next]\n",
    "    transfers.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    transfers['duration'] = (transfers.outtime - transfers.intime)/np.timedelta64(1,'h')\n",
    "    i += 1\n",
    "print('Number of loops',i)\n",
    "print('Rows merged:', original_len - len(transfers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7ccf9",
   "metadata": {},
   "source": [
    "### Invalid transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7751d1c1",
   "metadata": {},
   "source": [
    "Let's look at rows where the event (or destination careunit) is equal to the current careunit and there is a following transfer. Not all the rows here are actually rows that have been split, as sometimes the first row is followed by a row of a different patient. We will have to remove these rows as a transfer to the same careunit does not make much sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f099602a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T06:57:52.584902Z",
     "start_time": "2022-11-01T06:57:52.333141Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bool dataframe of rows equal to the indexes of the first row of the split rows\n",
    "split_rows = transfers[['subject_id','outtime','event']] == transfers[['subject_id','intime','careunit']]\\\n",
    ".shift(-1).rename(columns = {'intime':'outtime','careunit':'event'}) # Renaming so that the comparison works\n",
    "\n",
    "split_rows = split_rows.all(axis = 1) # Getting bool series that is true only when the three colums of booldf are true\n",
    "split_rows = split_rows & (transfers.event == transfers.careunit) # Filtering only those that have the same event as careunit\n",
    "# Outtime and event for these rows are now equal to the previous transfer\n",
    "transfers[['outtime','event']] = transfers[['outtime','event']].where(~split_rows,transfers[['outtime','event']].shift(-1))\n",
    "\n",
    "# Discarding second row of each merged row\n",
    "non_splits = ~split_rows.shift().fillna(False) # Nas formed during shift\n",
    "transfers = transfers[non_splits]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6073fcd0",
   "metadata": {},
   "source": [
    "Let's repeat to make sure that we have taken care of all the split rows (maybe some were split into 3 or more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "dc972373",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T06:58:19.092151Z",
     "start_time": "2022-11-01T06:58:18.932497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "split_rows = transfers[['subject_id','outtime','event']] == transfers[['subject_id','intime','careunit']]\\\n",
    ".shift(-1).rename(columns = {'intime':'outtime','careunit':'event'})\n",
    "split_rows = split_rows.all(axis = 1)\n",
    "split_rows = split_rows & (transfers.event == transfers.careunit)\n",
    "print(split_rows.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4fde2",
   "metadata": {},
   "source": [
    "No more split rows. Now we can check which events and careunits are the same without being continuous transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "eee7a82b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T06:58:24.862587Z",
     "start_time": "2022-11-01T06:58:24.337516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transfer_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>eventtype</th>\n",
       "      <th>careunit</th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>event</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>...</th>\n",
       "      <th>J</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>V</th>\n",
       "      <th>pres_number</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>39159098</td>\n",
       "      <td>10000048</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>ED</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>2126-11-22 18:45:00</td>\n",
       "      <td>2126-11-23 02:07:00</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>39891608</td>\n",
       "      <td>10000102</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>ED</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>2136-12-20 11:49:00</td>\n",
       "      <td>2136-12-20 13:56:00</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.116667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32522732</td>\n",
       "      <td>10000108</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>ED</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>2163-09-16 16:34:00</td>\n",
       "      <td>2163-09-16 18:13:00</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>39513268</td>\n",
       "      <td>10000108</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>ED</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>2163-09-24 16:14:00</td>\n",
       "      <td>2163-09-24 21:02:00</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>38081480</td>\n",
       "      <td>10000115</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>ED</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>2154-12-10 02:04:00</td>\n",
       "      <td>2154-12-10 05:59:00</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294046</th>\n",
       "      <td>35122662</td>\n",
       "      <td>19999659</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>ED</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>2182-01-07 20:42:00</td>\n",
       "      <td>2182-01-08 02:10:00</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294048</th>\n",
       "      <td>38224473</td>\n",
       "      <td>19999750</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>ED</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>2144-03-22 14:27:00</td>\n",
       "      <td>2144-03-22 18:47:00</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294049</th>\n",
       "      <td>38616897</td>\n",
       "      <td>19999782</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>ED</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>2156-11-09 21:37:00</td>\n",
       "      <td>2156-11-10 03:12:00</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294091</th>\n",
       "      <td>34751321</td>\n",
       "      <td>19999829</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>ED</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>2186-06-14 11:31:00</td>\n",
       "      <td>2186-06-14 14:56:00</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294098</th>\n",
       "      <td>32002659</td>\n",
       "      <td>19999914</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>ED</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>2158-12-24 11:41:00</td>\n",
       "      <td>2158-12-24 11:56:00</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313266 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         transfer_id  subject_id  hadm_id eventtype              careunit  \\\n",
       "11          39159098    10000048     <NA>        ED  Emergency Department   \n",
       "16          39891608    10000102     <NA>        ED  Emergency Department   \n",
       "17          32522732    10000108     <NA>        ED  Emergency Department   \n",
       "18          39513268    10000108     <NA>        ED  Emergency Department   \n",
       "20          38081480    10000115     <NA>        ED  Emergency Department   \n",
       "...              ...         ...      ...       ...                   ...   \n",
       "1294046     35122662    19999659     <NA>        ED  Emergency Department   \n",
       "1294048     38224473    19999750     <NA>        ED  Emergency Department   \n",
       "1294049     38616897    19999782     <NA>        ED  Emergency Department   \n",
       "1294091     34751321    19999829     <NA>        ED  Emergency Department   \n",
       "1294098     32002659    19999914     <NA>        ED  Emergency Department   \n",
       "\n",
       "                     intime             outtime                 event  A  B  \\\n",
       "11      2126-11-22 18:45:00 2126-11-23 02:07:00  Emergency Department  0  0   \n",
       "16      2136-12-20 11:49:00 2136-12-20 13:56:00  Emergency Department  0  0   \n",
       "17      2163-09-16 16:34:00 2163-09-16 18:13:00  Emergency Department  0  0   \n",
       "18      2163-09-24 16:14:00 2163-09-24 21:02:00  Emergency Department  0  0   \n",
       "20      2154-12-10 02:04:00 2154-12-10 05:59:00  Emergency Department  0  0   \n",
       "...                     ...                 ...                   ... .. ..   \n",
       "1294046 2182-01-07 20:42:00 2182-01-08 02:10:00  Emergency Department  0  0   \n",
       "1294048 2144-03-22 14:27:00 2144-03-22 18:47:00  Emergency Department  0  0   \n",
       "1294049 2156-11-09 21:37:00 2156-11-10 03:12:00  Emergency Department  0  0   \n",
       "1294091 2186-06-14 11:31:00 2186-06-14 14:56:00  Emergency Department  0  0   \n",
       "1294098 2158-12-24 11:41:00 2158-12-24 11:56:00  Emergency Department  0  0   \n",
       "\n",
       "         ...  J  L  M  N  P  R  S  V  pres_number  duration  \n",
       "11       ...  0  0  0  0  0  0  0  0            0  7.366667  \n",
       "16       ...  0  0  0  0  0  0  0  0            0  2.116667  \n",
       "17       ...  0  0  0  0  0  0  0  0            0  1.650000  \n",
       "18       ...  0  0  0  0  0  0  0  0            0  4.800000  \n",
       "20       ...  0  0  0  0  0  0  0  0            0  3.916667  \n",
       "...      ... .. .. .. .. .. .. .. ..          ...       ...  \n",
       "1294046  ...  0  0  0  0  0  0  0  0            0  5.466667  \n",
       "1294048  ...  0  0  1  0  0  0  0  0            1  4.333333  \n",
       "1294049  ...  0  0  0  0  0  0  0  0            0  5.583333  \n",
       "1294091  ...  0  0  0  0  0  0  0  0            0  3.416667  \n",
       "1294098  ...  0  0  0  0  0  0  0  0            0  0.250000  \n",
       "\n",
       "[313266 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 313266 entries, 11 to 1294098\n",
      "Data columns (total 24 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   transfer_id  313266 non-null  int64         \n",
      " 1   subject_id   313266 non-null  int64         \n",
      " 2   hadm_id      10727 non-null   Int64         \n",
      " 3   eventtype    313266 non-null  object        \n",
      " 4   careunit     313266 non-null  object        \n",
      " 5   intime       313266 non-null  datetime64[ns]\n",
      " 6   outtime      313266 non-null  datetime64[ns]\n",
      " 7   event        313266 non-null  object        \n",
      " 8   A            313266 non-null  uint16        \n",
      " 9   B            313266 non-null  uint16        \n",
      " 10  C            313266 non-null  uint16        \n",
      " 11  D            313266 non-null  uint16        \n",
      " 12  G            313266 non-null  uint16        \n",
      " 13  H            313266 non-null  uint16        \n",
      " 14  J            313266 non-null  uint16        \n",
      " 15  L            313266 non-null  uint16        \n",
      " 16  M            313266 non-null  uint16        \n",
      " 17  N            313266 non-null  uint16        \n",
      " 18  P            313266 non-null  uint16        \n",
      " 19  R            313266 non-null  uint16        \n",
      " 20  S            313266 non-null  uint16        \n",
      " 21  V            313266 non-null  uint16        \n",
      " 22  pres_number  313266 non-null  uint16        \n",
      " 23  duration     313266 non-null  float64       \n",
      "dtypes: Int64(1), datetime64[ns](2), float64(1), int64(2), object(3), uint16(15)\n",
      "memory usage: 33.2+ MB\n",
      "None\n",
      "         transfer_id    subject_id       hadm_id eventtype  \\\n",
      "count   3.132660e+05  3.132660e+05  1.072700e+04    313266   \n",
      "unique           NaN           NaN           NaN         3   \n",
      "top              NaN           NaN           NaN        ED   \n",
      "freq             NaN           NaN           NaN    302601   \n",
      "mean    3.500026e+07  1.500035e+07  2.496295e+07       NaN   \n",
      "min     3.000001e+07  1.000005e+07  2.000135e+07       NaN   \n",
      "25%     3.250287e+07  1.251237e+07  2.245338e+07       NaN   \n",
      "50%     3.499841e+07  1.500592e+07  2.497099e+07       NaN   \n",
      "75%     3.751494e+07  1.748720e+07  2.744953e+07       NaN   \n",
      "max     3.999995e+07  1.999991e+07  2.999766e+07       NaN   \n",
      "std     2.888864e+06  2.883081e+06  2.885483e+06       NaN   \n",
      "\n",
      "                    careunit                         intime  \\\n",
      "count                 313266                         313266   \n",
      "unique                    39                            NaN   \n",
      "top     Emergency Department                            NaN   \n",
      "freq                  302601                            NaN   \n",
      "mean                     NaN  2155-02-10 05:43:00.384781312   \n",
      "min                      NaN            2110-01-01 14:04:28   \n",
      "25%                      NaN  2135-01-14 03:10:15.000000512   \n",
      "50%                      NaN            2155-02-01 13:27:30   \n",
      "75%                      NaN  2175-04-09 12:18:44.999999488   \n",
      "max                      NaN            2212-01-24 09:59:00   \n",
      "std                      NaN                            NaN   \n",
      "\n",
      "                              outtime                 event              A  \\\n",
      "count                          313266                313266  313266.000000   \n",
      "unique                            NaN                    39            NaN   \n",
      "top                               NaN  Emergency Department            NaN   \n",
      "freq                              NaN                302601            NaN   \n",
      "mean    2155-02-10 15:12:03.859693568                   NaN       1.014441   \n",
      "min               2110-01-07 10:28:54                   NaN       0.000000   \n",
      "25%     2135-01-14 13:18:44.999999488                   NaN       0.000000   \n",
      "50%               2155-02-01 16:40:30                   NaN       0.000000   \n",
      "75%     2175-04-09 19:12:44.999999488                   NaN       1.000000   \n",
      "max               2212-01-24 13:42:00                   NaN     157.000000   \n",
      "std                               NaN                   NaN       2.944631   \n",
      "\n",
      "                    B  ...              J              L              M  \\\n",
      "count   313266.000000  ...  313266.000000  313266.000000  313266.000000   \n",
      "unique            NaN  ...            NaN            NaN            NaN   \n",
      "top               NaN  ...            NaN            NaN            NaN   \n",
      "freq              NaN  ...            NaN            NaN            NaN   \n",
      "mean         0.520219  ...       0.152299       0.061159       0.116687   \n",
      "min          0.000000  ...       0.000000       0.000000       0.000000   \n",
      "25%          0.000000  ...       0.000000       0.000000       0.000000   \n",
      "50%          0.000000  ...       0.000000       0.000000       0.000000   \n",
      "75%          0.000000  ...       0.000000       0.000000       0.000000   \n",
      "max        199.000000  ...      84.000000      49.000000      14.000000   \n",
      "std          2.708495  ...       0.757277       0.373270       0.395103   \n",
      "\n",
      "                    N              P              R              S  \\\n",
      "count   313266.000000  313266.000000  313266.000000  313266.000000   \n",
      "unique            NaN            NaN            NaN            NaN   \n",
      "top               NaN            NaN            NaN            NaN   \n",
      "freq              NaN            NaN            NaN            NaN   \n",
      "mean         0.834428       0.014125       0.412531       0.258927   \n",
      "min          0.000000       0.000000       0.000000       0.000000   \n",
      "25%          0.000000       0.000000       0.000000       0.000000   \n",
      "50%          0.000000       0.000000       0.000000       0.000000   \n",
      "75%          1.000000       0.000000       0.000000       0.000000   \n",
      "max         94.000000       8.000000      37.000000      76.000000   \n",
      "std          1.906246       0.130860       1.153834       1.049252   \n",
      "\n",
      "                    V    pres_number       duration  \n",
      "count   313266.000000  313266.000000  313266.000000  \n",
      "unique            NaN            NaN            NaN  \n",
      "top               NaN            NaN            NaN  \n",
      "freq              NaN            NaN            NaN  \n",
      "mean         0.144417       3.309191       6.783741  \n",
      "min          0.000000       0.000000     -22.733333  \n",
      "25%          0.000000       0.000000       2.833333  \n",
      "50%          0.000000       0.000000       4.316667  \n",
      "75%          0.000000       4.000000       6.283333  \n",
      "max        105.000000     612.000000    3212.902778  \n",
      "std          1.222692       7.579681      23.641066  \n",
      "\n",
      "[11 rows x 24 columns]\n",
      "count                       313266\n",
      "mean     0 days 09:29:03.474912694\n",
      "std      1 days 19:35:45.537334767\n",
      "min              -1 days +01:16:00\n",
      "25%                0 days 02:50:00\n",
      "50%                0 days 04:20:00\n",
      "75%                0 days 06:20:00\n",
      "max              146 days 23:18:59\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "non_transfers = transfers.careunit == transfers.event\n",
    "\n",
    "display(transfers[non_transfers])\n",
    "print(transfers[non_transfers].info())cd\n",
    "print(transfers[non_transfers].describe(include = 'all',datetime_is_numeric=True))\n",
    "print((transfers.outtime[non_transfers]-transfers.intime[non_transfers]).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8281d2",
   "metadata": {},
   "source": [
    "It is not clear why some transfers are from one department to the other, but these are not events that we can predict. We will drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1cdc0059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T06:58:44.441478Z",
     "start_time": "2022-11-01T06:58:44.102153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 896572 entries, 0 to 1294101\n",
      "Data columns (total 24 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   transfer_id  896572 non-null  int64         \n",
      " 1   subject_id   896572 non-null  int64         \n",
      " 2   hadm_id      852415 non-null  Int64         \n",
      " 3   eventtype    896572 non-null  object        \n",
      " 4   careunit     896572 non-null  object        \n",
      " 5   intime       896572 non-null  datetime64[ns]\n",
      " 6   outtime      896572 non-null  datetime64[ns]\n",
      " 7   event        896572 non-null  object        \n",
      " 8   A            896572 non-null  uint16        \n",
      " 9   B            896572 non-null  uint16        \n",
      " 10  C            896572 non-null  uint16        \n",
      " 11  D            896572 non-null  uint16        \n",
      " 12  G            896572 non-null  uint16        \n",
      " 13  H            896572 non-null  uint16        \n",
      " 14  J            896572 non-null  uint16        \n",
      " 15  L            896572 non-null  uint16        \n",
      " 16  M            896572 non-null  uint16        \n",
      " 17  N            896572 non-null  uint16        \n",
      " 18  P            896572 non-null  uint16        \n",
      " 19  R            896572 non-null  uint16        \n",
      " 20  S            896572 non-null  uint16        \n",
      " 21  V            896572 non-null  uint16        \n",
      " 22  pres_number  896572 non-null  uint16        \n",
      " 23  duration     896572 non-null  float64       \n",
      "dtypes: Int64(1), datetime64[ns](2), float64(1), int64(2), object(3), uint16(15)\n",
      "memory usage: 94.9+ MB\n"
     ]
    }
   ],
   "source": [
    "transfers = transfers[~non_transfers]\n",
    "transfers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4303a99",
   "metadata": {},
   "source": [
    "Finally, we can remove any transfers with duration equal to 0. There are some rows left with durations under> 0 but under 3 h, but we will leave these for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "61232e62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T07:03:17.990464Z",
     "start_time": "2022-11-01T07:03:17.867867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 896572 left from an original 1991704\n"
     ]
    }
   ],
   "source": [
    "transfers[~(transfers.duration==0)]\n",
    "print(f'There are {len(transfers)} left from an original 1991704')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ba3d0",
   "metadata": {},
   "source": [
    "## Demographic features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed83b3",
   "metadata": {},
   "source": [
    "We have decreased the number of our rows by more than half in this cleaning process. Let's join this table with features from other tables to get some demographic patient info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae2c4ee",
   "metadata": {},
   "source": [
    "### Patients table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c9fb2",
   "metadata": {},
   "source": [
    "We can start with the patients table. It contains gender, age and dod as relevant features. We can merge these on subject_id. The anchor_year is the random year assigned to the patient and the anchor_year_group is the real year range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a4aa6a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T20:18:13.243945Z",
     "start_time": "2022-11-06T20:18:12.686779Z"
    }
   },
   "outputs": [],
   "source": [
    "patients = pd.read_csv('G:/My Drive/Capstone-Data/MIMIC IV/patients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a42e1f36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T07:04:09.806332Z",
     "start_time": "2022-11-01T07:04:09.761233Z"
    }
   },
   "outputs": [],
   "source": [
    "patients['dod'] = pd.to_datetime(patients.dod, infer_datetime_format = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cfeced",
   "metadata": {},
   "source": [
    "Basic eda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1f97ae86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T07:04:09.886345Z",
     "start_time": "2022-11-01T07:04:09.808237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 315460 entries, 0 to 315459\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   subject_id         315460 non-null  int64         \n",
      " 1   gender             315460 non-null  object        \n",
      " 2   anchor_age         315460 non-null  int64         \n",
      " 3   anchor_year        315460 non-null  int64         \n",
      " 4   anchor_year_group  315460 non-null  object        \n",
      " 5   dod                30636 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(3), object(2)\n",
      "memory usage: 14.4+ MB\n",
      "None\n",
      "\n",
      "\n",
      "count                   30636\n",
      "unique                  19705\n",
      "top       2129-11-19 00:00:00\n",
      "freq                        7\n",
      "first     2104-12-24 00:00:00\n",
      "last      2212-01-22 00:00:00\n",
      "Name: dod, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(patients.info(show_counts = True))\n",
    "print('\\n')\n",
    "print(patients.dod.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34552d5",
   "metadata": {},
   "source": [
    "Okay, no null values (other that the patients that did not die) and nothing weird on date of death (dod). Let's look at info on gender and age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2241cdea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T07:04:10.085287Z",
     "start_time": "2022-11-01T07:04:09.919269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1768e\" style='display:inline'>\n",
       "  <caption>Patient genders</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1768e_level0_col0\" class=\"col_heading level0 col0\" >gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1768e_level0_row0\" class=\"row_heading level0 row0\" >F</th>\n",
       "      <td id=\"T_1768e_row0_col0\" class=\"data row0 col0\" >52.906549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1768e_level0_row1\" class=\"row_heading level0 row1\" >M</th>\n",
       "      <td id=\"T_1768e_row1_col0\" class=\"data row1 col0\" >47.093451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_710c6\" style='display:inline'>\n",
       "  <caption>% that died</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_710c6_level0_col0\" class=\"col_heading level0 col0\" >gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_710c6_level0_row0\" class=\"row_heading level0 row0\" >F</th>\n",
       "      <td id=\"T_710c6_row0_col0\" class=\"data row0 col0\" >8.775367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_710c6_level0_row1\" class=\"row_heading level0 row1\" >M</th>\n",
       "      <td id=\"T_710c6_row1_col0\" class=\"data row1 col0\" >10.763255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_d1529\" style='display:inline'>\n",
       "  <caption>Male and female age distributions</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d1529_level0_col0\" class=\"col_heading level0 col0\" >Males</th>\n",
       "      <th id=\"T_d1529_level0_col1\" class=\"col_heading level0 col1\" >Females</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d1529_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_d1529_row0_col0\" class=\"data row0 col0\" >148561.000000</td>\n",
       "      <td id=\"T_d1529_row0_col1\" class=\"data row0 col1\" >166899.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d1529_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_d1529_row1_col0\" class=\"data row1 col0\" >49.026588</td>\n",
       "      <td id=\"T_d1529_row1_col1\" class=\"data row1 col1\" >48.080480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d1529_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_d1529_row2_col0\" class=\"data row2 col0\" >20.146493</td>\n",
       "      <td id=\"T_d1529_row2_col1\" class=\"data row2 col1\" >21.515040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d1529_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_d1529_row3_col0\" class=\"data row3 col0\" >18.000000</td>\n",
       "      <td id=\"T_d1529_row3_col1\" class=\"data row3 col1\" >18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d1529_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_d1529_row4_col0\" class=\"data row4 col0\" >30.000000</td>\n",
       "      <td id=\"T_d1529_row4_col1\" class=\"data row4 col1\" >28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d1529_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_d1529_row5_col0\" class=\"data row5 col0\" >49.000000</td>\n",
       "      <td id=\"T_d1529_row5_col1\" class=\"data row5 col1\" >46.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d1529_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_d1529_row6_col0\" class=\"data row6 col0\" >65.000000</td>\n",
       "      <td id=\"T_d1529_row6_col1\" class=\"data row6 col1\" >65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d1529_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_d1529_row7_col0\" class=\"data row7 col0\" >91.000000</td>\n",
       "      <td id=\"T_d1529_row7_col1\" class=\"data row7 col1\" >91.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nachopy.display_sbs(\n",
    "                [patients.gender.value_counts(normalize = True)*100,\\\n",
    "\n",
    "                patients.loc[~patients.dod.isna(),'gender'].value_counts()/patients.gender.value_counts()*100,\\\n",
    "\n",
    "                pd.concat([patients.anchor_age[patients.gender == 'M'].describe().rename('Males'),\\\n",
    "                patients.anchor_age[patients.gender == 'F'].describe().rename('Females')], axis = 1)],\\\n",
    "\n",
    "                titles = ['Patient genders', '% that died', 'Male and female age distributions']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753bc93a",
   "metadata": {},
   "source": [
    "Okay, good for now. We do not have any death registered in our transfers tables. This should be an event different from discharge, as the features for patients that expired and patients that were discharged from the hospital will probably be significantly different.\n",
    "\n",
    "We will add the columns age, gender and dod to the transfers table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "81f5161f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T07:04:10.705823Z",
     "start_time": "2022-11-01T07:04:10.087288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 896572 entries, 0 to 896571\n",
      "Data columns (total 27 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   transfer_id  896572 non-null  int64         \n",
      " 1   subject_id   896572 non-null  int64         \n",
      " 2   hadm_id      852415 non-null  Int64         \n",
      " 3   eventtype    896572 non-null  object        \n",
      " 4   careunit     896572 non-null  object        \n",
      " 5   intime       896572 non-null  datetime64[ns]\n",
      " 6   outtime      896572 non-null  datetime64[ns]\n",
      " 7   event        896572 non-null  object        \n",
      " 8   A            896572 non-null  uint16        \n",
      " 9   B            896572 non-null  uint16        \n",
      " 10  C            896572 non-null  uint16        \n",
      " 11  D            896572 non-null  uint16        \n",
      " 12  G            896572 non-null  uint16        \n",
      " 13  H            896572 non-null  uint16        \n",
      " 14  J            896572 non-null  uint16        \n",
      " 15  L            896572 non-null  uint16        \n",
      " 16  M            896572 non-null  uint16        \n",
      " 17  N            896572 non-null  uint16        \n",
      " 18  P            896572 non-null  uint16        \n",
      " 19  R            896572 non-null  uint16        \n",
      " 20  S            896572 non-null  uint16        \n",
      " 21  V            896572 non-null  uint16        \n",
      " 22  pres_number  896572 non-null  uint16        \n",
      " 23  duration     896572 non-null  float64       \n",
      " 24  gender       896572 non-null  object        \n",
      " 25  anchor_age   896572 non-null  int64         \n",
      " 26  dod          228424 non-null  datetime64[ns]\n",
      "dtypes: Int64(1), datetime64[ns](3), float64(1), int64(3), object(4), uint16(15)\n",
      "memory usage: 115.4+ MB\n"
     ]
    }
   ],
   "source": [
    "transfers = pd.merge(transfers,patients[['subject_id','gender','anchor_age','dod']], on = 'subject_id',how = 'left')\n",
    "transfers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed6e1e",
   "metadata": {},
   "source": [
    "Less than 1% of patients in the transfers column died rather than get discharged. We will change the class to \"Deceased\" for these rows. We can drop the dod column and check for any missing values in the new columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca568d6",
   "metadata": {},
   "source": [
    "### Admissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c210ae75",
   "metadata": {},
   "source": [
    "Now let's check what we can add from admissions (private info not shown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "10fea209",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T07:04:12.674323Z",
     "start_time": "2022-11-01T07:04:10.724356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 454324 entries, 0 to 454323\n",
      "Data columns (total 15 columns):\n",
      " #   Column                Non-Null Count   Dtype \n",
      "---  ------                --------------   ----- \n",
      " 0   subject_id            454324 non-null  int64 \n",
      " 1   hadm_id               454324 non-null  int64 \n",
      " 2   admittime             454324 non-null  object\n",
      " 3   dischtime             454324 non-null  object\n",
      " 4   deathtime             9087 non-null    object\n",
      " 5   admission_type        454324 non-null  object\n",
      " 6   admission_location    454324 non-null  object\n",
      " 7   discharge_location    328519 non-null  object\n",
      " 8   insurance             454324 non-null  object\n",
      " 9   language              454324 non-null  object\n",
      " 10  marital_status        444608 non-null  object\n",
      " 11  race                  454324 non-null  object\n",
      " 12  edregtime             315460 non-null  object\n",
      " 13  edouttime             315460 non-null  object\n",
      " 14  hospital_expire_flag  454324 non-null  int64 \n",
      "dtypes: int64(3), object(12)\n",
      "memory usage: 52.0+ MB\n",
      "None\n",
      "subject_id              190279\n",
      "hadm_id                 454324\n",
      "admittime               446961\n",
      "dischtime               442600\n",
      "deathtime                 9086\n",
      "admission_type               9\n",
      "admission_location          11\n",
      "discharge_location          13\n",
      "insurance                    3\n",
      "language                     2\n",
      "marital_status               4\n",
      "race                        33\n",
      "edregtime               309765\n",
      "edouttime               309831\n",
      "hospital_expire_flag         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "admissions = pd.read_csv('G:/My Drive/Capstone-Data/MIMIC IV/admissions.csv')\n",
    "# display(admissions.head())\n",
    "print(admissions.info(show_counts = True))\n",
    "print(admissions.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fdbd0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T15:52:59.807450Z",
     "start_time": "2022-10-17T15:52:59.731986Z"
    }
   },
   "source": [
    "Okay, let's do a basic EDA of some variables that could be interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3ff3f058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T07:04:12.808247Z",
     "start_time": "2022-11-01T07:04:12.676325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_73b5e\" style='display:inline'>\n",
       "  <caption>admission_type value_counts</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_73b5e_level0_col0\" class=\"col_heading level0 col0\" >admission_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_73b5e_level0_row0\" class=\"row_heading level0 row0\" >EW EMER.</th>\n",
       "      <td id=\"T_73b5e_row0_col0\" class=\"data row0 col0\" >157487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73b5e_level0_row1\" class=\"row_heading level0 row1\" >EU OBSERVATION</th>\n",
       "      <td id=\"T_73b5e_row1_col0\" class=\"data row1 col0\" >100133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73b5e_level0_row2\" class=\"row_heading level0 row2\" >OBSERVATION ADMIT</th>\n",
       "      <td id=\"T_73b5e_row2_col0\" class=\"data row2 col0\" >55210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73b5e_level0_row3\" class=\"row_heading level0 row3\" >URGENT</th>\n",
       "      <td id=\"T_73b5e_row3_col0\" class=\"data row3 col0\" >47127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73b5e_level0_row4\" class=\"row_heading level0 row4\" >SURGICAL SAME DAY ADMISSION</th>\n",
       "      <td id=\"T_73b5e_row4_col0\" class=\"data row4 col0\" >35994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73b5e_level0_row5\" class=\"row_heading level0 row5\" >DIRECT EMER.</th>\n",
       "      <td id=\"T_73b5e_row5_col0\" class=\"data row5 col0\" >20584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73b5e_level0_row6\" class=\"row_heading level0 row6\" >DIRECT OBSERVATION</th>\n",
       "      <td id=\"T_73b5e_row6_col0\" class=\"data row6 col0\" >19698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73b5e_level0_row7\" class=\"row_heading level0 row7\" >ELECTIVE</th>\n",
       "      <td id=\"T_73b5e_row7_col0\" class=\"data row7 col0\" >11089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73b5e_level0_row8\" class=\"row_heading level0 row8\" >AMBULATORY OBSERVATION</th>\n",
       "      <td id=\"T_73b5e_row8_col0\" class=\"data row8 col0\" >7002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_631bd\" style='display:inline'>\n",
       "  <caption>discharge_location value_counts</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_631bd_level0_col0\" class=\"col_heading level0 col0\" >discharge_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_631bd_level0_row0\" class=\"row_heading level0 row0\" >HOME</th>\n",
       "      <td id=\"T_631bd_row0_col0\" class=\"data row0 col0\" >163664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_631bd_level0_row1\" class=\"row_heading level0 row1\" >HOME HEALTH CARE</th>\n",
       "      <td id=\"T_631bd_row1_col0\" class=\"data row1 col0\" >79506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_631bd_level0_row2\" class=\"row_heading level0 row2\" >SKILLED NURSING FACILITY</th>\n",
       "      <td id=\"T_631bd_row2_col0\" class=\"data row2 col0\" >45280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_631bd_level0_row3\" class=\"row_heading level0 row3\" >REHAB</th>\n",
       "      <td id=\"T_631bd_row3_col0\" class=\"data row3 col0\" >11025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_631bd_level0_row4\" class=\"row_heading level0 row4\" >DIED</th>\n",
       "      <td id=\"T_631bd_row4_col0\" class=\"data row4 col0\" >8997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_631bd_level0_row5\" class=\"row_heading level0 row5\" >CHRONIC/LONG TERM ACUTE CARE</th>\n",
       "      <td id=\"T_631bd_row5_col0\" class=\"data row5 col0\" >7531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_631bd_level0_row6\" class=\"row_heading level0 row6\" >HOSPICE</th>\n",
       "      <td id=\"T_631bd_row6_col0\" class=\"data row6 col0\" >3649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_631bd_level0_row7\" class=\"row_heading level0 row7\" >AGAINST ADVICE</th>\n",
       "      <td id=\"T_631bd_row7_col0\" class=\"data row7 col0\" >2703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_631bd_level0_row8\" class=\"row_heading level0 row8\" >PSYCH FACILITY</th>\n",
       "      <td id=\"T_631bd_row8_col0\" class=\"data row8 col0\" >2405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_631bd_level0_row9\" class=\"row_heading level0 row9\" >ACUTE HOSPITAL</th>\n",
       "      <td id=\"T_631bd_row9_col0\" class=\"data row9 col0\" >1702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_631bd_level0_row10\" class=\"row_heading level0 row10\" >OTHER FACILITY</th>\n",
       "      <td id=\"T_631bd_row10_col0\" class=\"data row10 col0\" >1428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_631bd_level0_row11\" class=\"row_heading level0 row11\" >ASSISTED LIVING</th>\n",
       "      <td id=\"T_631bd_row11_col0\" class=\"data row11 col0\" >583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_631bd_level0_row12\" class=\"row_heading level0 row12\" >HEALTHCARE FACILITY</th>\n",
       "      <td id=\"T_631bd_row12_col0\" class=\"data row12 col0\" >46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_5f702\" style='display:inline'>\n",
       "  <caption>insurance value_counts</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5f702_level0_col0\" class=\"col_heading level0 col0\" >insurance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5f702_level0_row0\" class=\"row_heading level0 row0\" >Other</th>\n",
       "      <td id=\"T_5f702_row0_col0\" class=\"data row0 col0\" >241613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5f702_level0_row1\" class=\"row_heading level0 row1\" >Medicare</th>\n",
       "      <td id=\"T_5f702_row1_col0\" class=\"data row1 col0\" >168963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5f702_level0_row2\" class=\"row_heading level0 row2\" >Medicaid</th>\n",
       "      <td id=\"T_5f702_row2_col0\" class=\"data row2 col0\" >43748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_a9ec4\" style='display:inline'>\n",
       "  <caption>language value_counts</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a9ec4_level0_col0\" class=\"col_heading level0 col0\" >language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a9ec4_level0_row0\" class=\"row_heading level0 row0\" >ENGLISH</th>\n",
       "      <td id=\"T_a9ec4_row0_col0\" class=\"data row0 col0\" >409153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9ec4_level0_row1\" class=\"row_heading level0 row1\" >?</th>\n",
       "      <td id=\"T_a9ec4_row1_col0\" class=\"data row1 col0\" >45171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_3b60d\" style='display:inline'>\n",
       "  <caption>marital_status value_counts</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3b60d_level0_col0\" class=\"col_heading level0 col0\" >marital_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3b60d_level0_row0\" class=\"row_heading level0 row0\" >MARRIED</th>\n",
       "      <td id=\"T_3b60d_row0_col0\" class=\"data row0 col0\" >191114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b60d_level0_row1\" class=\"row_heading level0 row1\" >SINGLE</th>\n",
       "      <td id=\"T_3b60d_row1_col0\" class=\"data row1 col0\" >171996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b60d_level0_row2\" class=\"row_heading level0 row2\" >WIDOWED</th>\n",
       "      <td id=\"T_3b60d_row2_col0\" class=\"data row2 col0\" >48334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b60d_level0_row3\" class=\"row_heading level0 row3\" >DIVORCED</th>\n",
       "      <td id=\"T_3b60d_row3_col0\" class=\"data row3 col0\" >33164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_72a9d\" style='display:inline'>\n",
       "  <caption>race value_counts</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_72a9d_level0_col0\" class=\"col_heading level0 col0\" >race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row0\" class=\"row_heading level0 row0\" >WHITE</th>\n",
       "      <td id=\"T_72a9d_row0_col0\" class=\"data row0 col0\" >287313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row1\" class=\"row_heading level0 row1\" >BLACK/AFRICAN AMERICAN</th>\n",
       "      <td id=\"T_72a9d_row1_col0\" class=\"data row1 col0\" >63195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row2\" class=\"row_heading level0 row2\" >OTHER</th>\n",
       "      <td id=\"T_72a9d_row2_col0\" class=\"data row2 col0\" >15865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row3\" class=\"row_heading level0 row3\" >UNKNOWN</th>\n",
       "      <td id=\"T_72a9d_row3_col0\" class=\"data row3 col0\" >11237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row4\" class=\"row_heading level0 row4\" >HISPANIC/LATINO - PUERTO RICAN</th>\n",
       "      <td id=\"T_72a9d_row4_col0\" class=\"data row4 col0\" >8591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row5\" class=\"row_heading level0 row5\" >WHITE - OTHER EUROPEAN</th>\n",
       "      <td id=\"T_72a9d_row5_col0\" class=\"data row5 col0\" >8379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row6\" class=\"row_heading level0 row6\" >HISPANIC OR LATINO</th>\n",
       "      <td id=\"T_72a9d_row6_col0\" class=\"data row6 col0\" >8250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row7\" class=\"row_heading level0 row7\" >ASIAN</th>\n",
       "      <td id=\"T_72a9d_row7_col0\" class=\"data row7 col0\" >6520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row8\" class=\"row_heading level0 row8\" >ASIAN - CHINESE</th>\n",
       "      <td id=\"T_72a9d_row8_col0\" class=\"data row8 col0\" >5865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row9\" class=\"row_heading level0 row9\" >WHITE - RUSSIAN</th>\n",
       "      <td id=\"T_72a9d_row9_col0\" class=\"data row9 col0\" >5307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row10\" class=\"row_heading level0 row10\" >BLACK/CAPE VERDEAN</th>\n",
       "      <td id=\"T_72a9d_row10_col0\" class=\"data row10 col0\" >5028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row11\" class=\"row_heading level0 row11\" >HISPANIC/LATINO - DOMINICAN</th>\n",
       "      <td id=\"T_72a9d_row11_col0\" class=\"data row11 col0\" >4622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row12\" class=\"row_heading level0 row12\" >BLACK/CARIBBEAN ISLAND</th>\n",
       "      <td id=\"T_72a9d_row12_col0\" class=\"data row12 col0\" >2801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row13\" class=\"row_heading level0 row13\" >BLACK/AFRICAN</th>\n",
       "      <td id=\"T_72a9d_row13_col0\" class=\"data row13 col0\" >2676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row14\" class=\"row_heading level0 row14\" >PATIENT DECLINED TO ANSWER</th>\n",
       "      <td id=\"T_72a9d_row14_col0\" class=\"data row14 col0\" >1845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row15\" class=\"row_heading level0 row15\" >UNABLE TO OBTAIN</th>\n",
       "      <td id=\"T_72a9d_row15_col0\" class=\"data row15 col0\" >1684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72a9d_level0_row16\" class=\"row_heading level0 row16\" >PORTUGUESE</th>\n",
       "      <td id=\"T_72a9d_row16_col0\" class=\"data row16 col0\" >1567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_14354\" style='display:inline'>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_14354_level0_col0\" class=\"col_heading level0 col0\" >race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_14354_level0_row0\" class=\"row_heading level0 row0\" >ASIAN - SOUTH EAST ASIAN</th>\n",
       "      <td id=\"T_14354_row0_col0\" class=\"data row0 col0\" >1527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14354_level0_row1\" class=\"row_heading level0 row1\" >HISPANIC/LATINO - GUATEMALAN</th>\n",
       "      <td id=\"T_14354_row1_col0\" class=\"data row1 col0\" >1389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14354_level0_row2\" class=\"row_heading level0 row2\" >ASIAN - ASIAN INDIAN</th>\n",
       "      <td id=\"T_14354_row2_col0\" class=\"data row2 col0\" >1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14354_level0_row3\" class=\"row_heading level0 row3\" >WHITE - EASTERN EUROPEAN</th>\n",
       "      <td id=\"T_14354_row3_col0\" class=\"data row3 col0\" >1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14354_level0_row4\" class=\"row_heading level0 row4\" >WHITE - BRAZILIAN</th>\n",
       "      <td id=\"T_14354_row4_col0\" class=\"data row4 col0\" >1176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14354_level0_row5\" class=\"row_heading level0 row5\" >AMERICAN INDIAN/ALASKA NATIVE</th>\n",
       "      <td id=\"T_14354_row5_col0\" class=\"data row5 col0\" >984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14354_level0_row6\" class=\"row_heading level0 row6\" >HISPANIC/LATINO - SALVADORAN</th>\n",
       "      <td id=\"T_14354_row6_col0\" class=\"data row6 col0\" >939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14354_level0_row7\" class=\"row_heading level0 row7\" >HISPANIC/LATINO - MEXICAN</th>\n",
       "      <td id=\"T_14354_row7_col0\" class=\"data row7 col0\" >693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14354_level0_row8\" class=\"row_heading level0 row8\" >HISPANIC/LATINO - COLUMBIAN</th>\n",
       "      <td id=\"T_14354_row8_col0\" class=\"data row8 col0\" >683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14354_level0_row9\" class=\"row_heading level0 row9\" >MULTIPLE RACE/ETHNICITY</th>\n",
       "      <td id=\"T_14354_row9_col0\" class=\"data row9 col0\" >595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14354_level0_row10\" class=\"row_heading level0 row10\" >HISPANIC/LATINO - HONDURAN</th>\n",
       "      <td id=\"T_14354_row10_col0\" class=\"data row10 col0\" >574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14354_level0_row11\" class=\"row_heading level0 row11\" >ASIAN - KOREAN</th>\n",
       "      <td id=\"T_14354_row11_col0\" class=\"data row11 col0\" >544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14354_level0_row12\" class=\"row_heading level0 row12\" >SOUTH AMERICAN</th>\n",
       "      <td id=\"T_14354_row12_col0\" class=\"data row12 col0\" >542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14354_level0_row13\" class=\"row_heading level0 row13\" >HISPANIC/LATINO - CUBAN</th>\n",
       "      <td id=\"T_14354_row13_col0\" class=\"data row13 col0\" >529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14354_level0_row14\" class=\"row_heading level0 row14\" >HISPANIC/LATINO - CENTRAL AMERICAN</th>\n",
       "      <td id=\"T_14354_row14_col0\" class=\"data row14 col0\" >481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14354_level0_row15\" class=\"row_heading level0 row15\" >NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER</th>\n",
       "      <td id=\"T_14354_row15_col0\" class=\"data row15 col0\" >405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = ['admission_type','discharge_location','insurance','language','marital_status','race']\n",
    "nachopy.display_sbs([admissions[col].value_counts() for col in columns],max_rows = 17, suffix = 'value_counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e40379a",
   "metadata": {},
   "source": [
    "Admission type seems quite relevant. Discharge location also looks relevant and should probably be used to update the class (\"event\"), at least the discharges. Insurance relevant as well, as medicaid and medicare patients could have different outcomes than privately-insured. English and marital status will be added for now, although probably little relevance. Race may be relevant, could be added simply as a dummy column (1 if white, 0 otherwise). We will join now and adjust later during the EDA. \n",
    "\n",
    "Because we have some missing values on the admissions IDs (hadm_id), we will fill some of these features using subject_id (race, insurance and language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4e2fb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T07:04:14.345189Z",
     "start_time": "2022-11-01T07:04:12.810247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Storing the hadm_id column as is (with NaNs)\n",
    "hadm_nas = transfers.hadm_id.isna()\n",
    "transfers['hadm_id'] transfers.hadm_id.fillna(0) # For the merge only\n",
    "columns.append('hadm_id')\n",
    "\n",
    "# Joining to transfers\n",
    "transfers = pd.merge(transfers,admissions[columns], how = 'left', on = 'hadm_id')\n",
    "transfers['hadm_id'] = transfers.hadm_id.where(~hadm_nas) # Setting missing values as NaNs again\n",
    "transfers.rename(columns = {'hadm_id_nans':'hadm_id'},inplace = True)\n",
    "transfers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e6e8f",
   "metadata": {},
   "source": [
    "To fill the missing values, we can see if there are any subject_id that has a hadm_id on the admissions table but not on the transfers table. For race insurance and language there are no missing values on the admissions table, so we can try to retrieve them by updating using subject_id for the first occurence of these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7debb8dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T07:04:15.856546Z",
     "start_time": "2022-11-01T07:04:14.347802Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 896572 entries, 10000032 to 19999987\n",
      "Data columns (total 33 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   transfer_id         896572 non-null  int64         \n",
      " 1   subject_id          896572 non-null  int64         \n",
      " 2   eventtype           896572 non-null  object        \n",
      " 3   careunit            896572 non-null  object        \n",
      " 4   intime              896572 non-null  datetime64[ns]\n",
      " 5   outtime             896572 non-null  datetime64[ns]\n",
      " 6   event               896572 non-null  object        \n",
      " 7   A                   896572 non-null  uint16        \n",
      " 8   B                   896572 non-null  uint16        \n",
      " 9   C                   896572 non-null  uint16        \n",
      " 10  D                   896572 non-null  uint16        \n",
      " 11  G                   896572 non-null  uint16        \n",
      " 12  H                   896572 non-null  uint16        \n",
      " 13  J                   896572 non-null  uint16        \n",
      " 14  L                   896572 non-null  uint16        \n",
      " 15  M                   896572 non-null  uint16        \n",
      " 16  N                   896572 non-null  uint16        \n",
      " 17  P                   896572 non-null  uint16        \n",
      " 18  R                   896572 non-null  uint16        \n",
      " 19  S                   896572 non-null  uint16        \n",
      " 20  V                   896572 non-null  uint16        \n",
      " 21  pres_number         896572 non-null  uint16        \n",
      " 22  duration            896572 non-null  float64       \n",
      " 23  gender              896572 non-null  object        \n",
      " 24  anchor_age          896572 non-null  int64         \n",
      " 25  dod                 228424 non-null  datetime64[ns]\n",
      " 26  hadm_id             878045 non-null  object        \n",
      " 27  admission_type      878045 non-null  object        \n",
      " 28  discharge_location  815583 non-null  object        \n",
      " 29  insurance           878045 non-null  object        \n",
      " 30  language            878045 non-null  object        \n",
      " 31  marital_status      859826 non-null  object        \n",
      " 32  race                878045 non-null  object        \n",
      "dtypes: datetime64[ns](3), float64(1), int64(3), object(11), uint16(15)\n",
      "memory usage: 155.6+ MB\n"
     ]
    }
   ],
   "source": [
    "columns.append('subject_id')\n",
    "missing_rows = admissions[columns].groupby('subject_id').first()\n",
    "\n",
    "transfers.reset_index(drop = False)\n",
    "transfers.set_index('subject_id',drop = False,inplace = True)\n",
    "transfers.update(missing_rows)\n",
    "transfers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40256078",
   "metadata": {},
   "source": [
    "We have increased our number of hadm_ids and also the other new columns. We could search the database to fill the missing values for some of the demographic features, but the improvement would be minimal. Plus, some of these features are probably not very relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "803b5617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T07:04:16.405083Z",
     "start_time": "2022-11-01T07:04:15.858547Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>arrival_transport</th>\n",
       "      <th>disposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853.0</td>\n",
       "      <td>33258284</td>\n",
       "      <td>2180-05-06 19:17:00</td>\n",
       "      <td>2180-05-06 23:30:00</td>\n",
       "      <td>AMBULANCE</td>\n",
       "      <td>ADMITTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22841357.0</td>\n",
       "      <td>38112554</td>\n",
       "      <td>2180-06-26 15:54:00</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "      <td>AMBULANCE</td>\n",
       "      <td>ADMITTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>25742920.0</td>\n",
       "      <td>35968195</td>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>AMBULANCE</td>\n",
       "      <td>ADMITTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034.0</td>\n",
       "      <td>32952584</td>\n",
       "      <td>2180-07-22 16:24:00</td>\n",
       "      <td>2180-07-23 05:54:00</td>\n",
       "      <td>AMBULANCE</td>\n",
       "      <td>HOME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034.0</td>\n",
       "      <td>39399961</td>\n",
       "      <td>2180-07-23 05:54:00</td>\n",
       "      <td>2180-07-23 14:00:00</td>\n",
       "      <td>AMBULANCE</td>\n",
       "      <td>ADMITTED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id     hadm_id   stay_id               intime              outtime  \\\n",
       "0    10000032  22595853.0  33258284  2180-05-06 19:17:00  2180-05-06 23:30:00   \n",
       "1    10000032  22841357.0  38112554  2180-06-26 15:54:00  2180-06-26 21:31:00   \n",
       "2    10000032  25742920.0  35968195  2180-08-05 20:58:00  2180-08-06 01:44:00   \n",
       "3    10000032  29079034.0  32952584  2180-07-22 16:24:00  2180-07-23 05:54:00   \n",
       "4    10000032  29079034.0  39399961  2180-07-23 05:54:00  2180-07-23 14:00:00   \n",
       "\n",
       "  arrival_transport disposition  \n",
       "0         AMBULANCE    ADMITTED  \n",
       "1         AMBULANCE    ADMITTED  \n",
       "2         AMBULANCE    ADMITTED  \n",
       "3         AMBULANCE        HOME  \n",
       "4         AMBULANCE    ADMITTED  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edstays = pd.read_csv('G:/My Drive/Capstone-Data/MIMIC IV/ed/edstays.csv')\n",
    "edstays.head().drop(columns = ['race','gender']) # We will not show sensitive information that can help reidentify patients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e651aabd",
   "metadata": {},
   "source": [
    "The ED collects different data than the hospital, which is unfortunate. We will leave these features as they are for now.carry out analysis regardless. It appears we can add race (we can just group by subject_id and then update the column with the first occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "30c8f8ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T07:04:17.479168Z",
     "start_time": "2022-11-01T07:04:16.407956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 896572 entries, 10000032 to 19999987\n",
      "Data columns (total 33 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   transfer_id         896572 non-null  int64         \n",
      " 1   subject_id          896572 non-null  int64         \n",
      " 2   eventtype           896572 non-null  object        \n",
      " 3   careunit            896572 non-null  object        \n",
      " 4   intime              896572 non-null  datetime64[ns]\n",
      " 5   outtime             896572 non-null  datetime64[ns]\n",
      " 6   event               896572 non-null  object        \n",
      " 7   A                   896572 non-null  uint16        \n",
      " 8   B                   896572 non-null  uint16        \n",
      " 9   C                   896572 non-null  uint16        \n",
      " 10  D                   896572 non-null  uint16        \n",
      " 11  G                   896572 non-null  uint16        \n",
      " 12  H                   896572 non-null  uint16        \n",
      " 13  J                   896572 non-null  uint16        \n",
      " 14  L                   896572 non-null  uint16        \n",
      " 15  M                   896572 non-null  uint16        \n",
      " 16  N                   896572 non-null  uint16        \n",
      " 17  P                   896572 non-null  uint16        \n",
      " 18  R                   896572 non-null  uint16        \n",
      " 19  S                   896572 non-null  uint16        \n",
      " 20  V                   896572 non-null  uint16        \n",
      " 21  pres_number         896572 non-null  uint16        \n",
      " 22  duration            896572 non-null  float64       \n",
      " 23  gender              896572 non-null  object        \n",
      " 24  anchor_age          896572 non-null  int64         \n",
      " 25  dod                 228424 non-null  datetime64[ns]\n",
      " 26  hadm_id             878045 non-null  object        \n",
      " 27  admission_type      878045 non-null  object        \n",
      " 28  discharge_location  815583 non-null  object        \n",
      " 29  insurance           878045 non-null  object        \n",
      " 30  language            878045 non-null  object        \n",
      " 31  marital_status      859826 non-null  object        \n",
      " 32  race                890867 non-null  object        \n",
      "dtypes: datetime64[ns](3), float64(1), int64(3), object(11), uint16(15)\n",
      "memory usage: 155.6+ MB\n"
     ]
    }
   ],
   "source": [
    "missing_rows = edstays[['hadm_id','race','subject_id']].groupby('subject_id').first()\n",
    "transfers.update(missing_rows)\n",
    "transfers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccabf69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T13:20:45.717303Z",
     "start_time": "2022-10-20T13:20:45.380996Z"
    }
   },
   "source": [
    "We are done with all the merging so for now, we will leave transfers_id as index, as it is a unique identifier of the rows. We will also save the df in our pickle jar as version 1 of our dataset. We have carried out an EDA and feature engineering in another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5d9320dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T07:04:17.859732Z",
     "start_time": "2022-11-01T07:04:17.550384Z"
    }
   },
   "outputs": [],
   "source": [
    "transfers.to_pickle('G:/My Drive/Capstone-Data/pickles/transfers_v1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c1061b",
   "metadata": {},
   "source": [
    "## Joining with ATC level 2 prescriptions features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b0e1be",
   "metadata": {},
   "source": [
    "During the model optimization and feature selection phase, we have decided to increase the granularity of the prescriptions to add new features. Let's make a dataframe adding the transfer IDs to the prescriptions table so that we con join the new categories to the split datasets on the model optimization page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "337bd592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T19:59:22.812426Z",
     "start_time": "2022-11-05T19:59:22.068505Z"
    }
   },
   "outputs": [],
   "source": [
    "transfers = pd.read_pickle('G:/My Drive/Capstone-Data/pickles/transfers_v1.pkl')\n",
    "pres = pd.read_pickle('G:/My Drive/Capstone-Data/pickles/prescriptions_v2.pkl')\n",
    "lvl2_cat = list(pres.columns[8:]) # Selecting categoricla columns for merging\n",
    "pres[lvl2_cat] = pres[lvl2_cat].where(pres.A01>0,0) # Setting -1 values (null) as NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f84701",
   "metadata": {},
   "source": [
    "We can apply our previous prescriptions merging script again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12b03f2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T20:08:26.689116Z",
     "start_time": "2022-11-05T19:59:35.298203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop 50/50\n"
     ]
    }
   ],
   "source": [
    "# Initialising variables for loop\n",
    "# Getting only necessary columns for the row selection\n",
    "transfers = transfers[['subject_id','intime','outtime','transfer_id']].set_index('subject_id').sort_index()\n",
    "# Adding hadm_id because it will be useful to fill hadm_id column on transfers\n",
    "pres = pres.drop(columns =[ 'hadm_id','drug_type','drug','formulary_drug_cd','gsn','ndc']).set_index('subject_id').sort_index() \n",
    "\n",
    "n = 50\n",
    "\n",
    "df_merged = pd.DataFrame(columns = lvl2_cat)\n",
    "\n",
    "len_r = len(pres)\n",
    "max_l = transfers.index.max()\n",
    "max_r = pres.index.max()\n",
    "\n",
    "inc = round(len(pres)/n)\n",
    "ind_r = copy.copy(inc)\n",
    "sub_1 = 0\n",
    "sub_2 = 0\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    ind_r += inc \n",
    "    if ind_r < len_r:\n",
    "        sub_2 = pres.iloc[ind_r].name # Getting last index in increment \n",
    "    else:\n",
    "        sub_2 = max_l # Getting last subject on left (transfers) df when reaching the end of the df\n",
    "    \n",
    "    pres_chunk = pres.loc[sub_1:sub_2]\n",
    "    transfers_chunk = transfers.loc[sub_1:sub_2]\n",
    "    # Inner join as any rows without prescriptions are lost anyway (will be recovered later)\n",
    "    df_merging = pd.merge(transfers_chunk,pres_chunk, on = 'subject_id') \n",
    "\n",
    "    # Selecting only rows where the prescription is between starttime and outtime \n",
    "    valid_rows = (df_merging.starttime>=df_merging.intime) & (df_merging.starttime<df_merging.outtime)\n",
    "    \n",
    "    df_merging = df_merging[valid_rows].reset_index(drop = True)\n",
    "    # Retrieving hadm_id from prescriptions to merge later\n",
    "    df_merging = df_merging.groupby('transfer_id')[lvl2_cat].sum() # We sum to frequency encode prescriptions\n",
    "\n",
    "    df_merged = pd.concat([df_merged, df_merging],axis = 0)\n",
    "    \n",
    "    sub_1 = sub_2 + 1\n",
    "    \n",
    "    print(f'Loop {i + 1}/{n}',end='\\r')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191f73a",
   "metadata": {},
   "source": [
    "Let's check if there are any empty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a60de9bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T21:05:28.399300Z",
     "start_time": "2022-11-05T21:05:16.169195Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36 columns with 0 prescriptions, we can remove these\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 736824 entries, 30000417 to 39999906\n",
      "Data columns (total 51 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   A01     736824 non-null  object\n",
      " 1   A02     736824 non-null  object\n",
      " 2   A06     736824 non-null  object\n",
      " 3   A07     736824 non-null  object\n",
      " 4   A09     736824 non-null  object\n",
      " 5   A11     736824 non-null  object\n",
      " 6   A12     736824 non-null  object\n",
      " 7   A16     736824 non-null  object\n",
      " 8   B01     736824 non-null  object\n",
      " 9   B02     736824 non-null  object\n",
      " 10  B03     736824 non-null  object\n",
      " 11  B05     736824 non-null  object\n",
      " 12  C01     736824 non-null  object\n",
      " 13  C03     736824 non-null  object\n",
      " 14  C04     736824 non-null  object\n",
      " 15  C05     736824 non-null  object\n",
      " 16  C10     736824 non-null  object\n",
      " 17  D01     736824 non-null  object\n",
      " 18  D02     736824 non-null  object\n",
      " 19  D03     736824 non-null  object\n",
      " 20  D04     736824 non-null  object\n",
      " 21  D06     736824 non-null  object\n",
      " 22  D07     736824 non-null  object\n",
      " 23  D08     736824 non-null  object\n",
      " 24  D09     736824 non-null  object\n",
      " 25  D10     736824 non-null  object\n",
      " 26  D11     736824 non-null  object\n",
      " 27  G01     736824 non-null  object\n",
      " 28  H02     736824 non-null  object\n",
      " 29  J01     736824 non-null  object\n",
      " 30  J02     736824 non-null  object\n",
      " 31  J05     736824 non-null  object\n",
      " 32  M01     736824 non-null  object\n",
      " 33  M03     736824 non-null  object\n",
      " 34  N01     736824 non-null  object\n",
      " 35  N02     736824 non-null  object\n",
      " 36  N05     736824 non-null  object\n",
      " 37  N06     736824 non-null  object\n",
      " 38  N07     736824 non-null  object\n",
      " 39  P01     736824 non-null  object\n",
      " 40  P03     736824 non-null  object\n",
      " 41  R01     736824 non-null  object\n",
      " 42  R02     736824 non-null  object\n",
      " 43  R03     736824 non-null  object\n",
      " 44  R05     736824 non-null  object\n",
      " 45  R06     736824 non-null  object\n",
      " 46  S01     736824 non-null  object\n",
      " 47  S02     736824 non-null  object\n",
      " 48  S03     736824 non-null  object\n",
      " 49  V04     736824 non-null  object\n",
      " 50  V06     736824 non-null  object\n",
      "dtypes: object(51)\n",
      "memory usage: 292.3+ MB\n"
     ]
    }
   ],
   "source": [
    "valid_cols = df_merged.max()>0\n",
    "print(f'There are {(~valid_cols).sum()} columns with 0 prescriptions, we can remove these')\n",
    "df_merged = df_merged[df_merged.columns[valid_cols]]\n",
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cee426",
   "metadata": {},
   "source": [
    "There was, we could remove 36 columns out of the total 50. The data type changed to object, we can change it back to int. Let's see which type we could use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e804927",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T21:09:29.029622Z",
     "start_time": "2022-11-05T21:09:24.705926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.max().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571929aa",
   "metadata": {},
   "source": [
    "We could do int8 (max below 128). We will use uint8 instead because it takes the same space and we are not expecting any negative values. We will save a pickle to eat it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29fb8504",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-05T21:14:07.445624Z",
     "start_time": "2022-11-05T21:14:07.248839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 736824 entries, 30000417 to 39999906\n",
      "Data columns (total 51 columns):\n",
      " #   Column  Non-Null Count   Dtype\n",
      "---  ------  --------------   -----\n",
      " 0   A01     736824 non-null  uint8\n",
      " 1   A02     736824 non-null  uint8\n",
      " 2   A06     736824 non-null  uint8\n",
      " 3   A07     736824 non-null  uint8\n",
      " 4   A09     736824 non-null  uint8\n",
      " 5   A11     736824 non-null  uint8\n",
      " 6   A12     736824 non-null  uint8\n",
      " 7   A16     736824 non-null  uint8\n",
      " 8   B01     736824 non-null  uint8\n",
      " 9   B02     736824 non-null  uint8\n",
      " 10  B03     736824 non-null  uint8\n",
      " 11  B05     736824 non-null  uint8\n",
      " 12  C01     736824 non-null  uint8\n",
      " 13  C03     736824 non-null  uint8\n",
      " 14  C04     736824 non-null  uint8\n",
      " 15  C05     736824 non-null  uint8\n",
      " 16  C10     736824 non-null  uint8\n",
      " 17  D01     736824 non-null  uint8\n",
      " 18  D02     736824 non-null  uint8\n",
      " 19  D03     736824 non-null  uint8\n",
      " 20  D04     736824 non-null  uint8\n",
      " 21  D06     736824 non-null  uint8\n",
      " 22  D07     736824 non-null  uint8\n",
      " 23  D08     736824 non-null  uint8\n",
      " 24  D09     736824 non-null  uint8\n",
      " 25  D10     736824 non-null  uint8\n",
      " 26  D11     736824 non-null  uint8\n",
      " 27  G01     736824 non-null  uint8\n",
      " 28  H02     736824 non-null  uint8\n",
      " 29  J01     736824 non-null  uint8\n",
      " 30  J02     736824 non-null  uint8\n",
      " 31  J05     736824 non-null  uint8\n",
      " 32  M01     736824 non-null  uint8\n",
      " 33  M03     736824 non-null  uint8\n",
      " 34  N01     736824 non-null  uint8\n",
      " 35  N02     736824 non-null  uint8\n",
      " 36  N05     736824 non-null  uint8\n",
      " 37  N06     736824 non-null  uint8\n",
      " 38  N07     736824 non-null  uint8\n",
      " 39  P01     736824 non-null  uint8\n",
      " 40  P03     736824 non-null  uint8\n",
      " 41  R01     736824 non-null  uint8\n",
      " 42  R02     736824 non-null  uint8\n",
      " 43  R03     736824 non-null  uint8\n",
      " 44  R05     736824 non-null  uint8\n",
      " 45  R06     736824 non-null  uint8\n",
      " 46  S01     736824 non-null  uint8\n",
      " 47  S02     736824 non-null  uint8\n",
      " 48  S03     736824 non-null  uint8\n",
      " 49  V04     736824 non-null  uint8\n",
      " 50  V06     736824 non-null  uint8\n",
      "dtypes: uint8(51)\n",
      "memory usage: 41.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_merged = df_merged.astype('uint8')\n",
    "print(df_merged.info())\n",
    "df_merged.to_pickle('G:/My Drive/Capstone-Data/pickles/trans_pres.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec3d4fd",
   "metadata": {},
   "source": [
    "We are down to 51 features and 41.5 MB, which should make every merging very easy. These data will be used directly on the model optimization notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone_env]",
   "language": "python",
   "name": "conda-env-capstone_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 388.844,
   "position": {
    "height": "410.837px",
    "left": "395.2px",
    "right": "20px",
    "top": "119px",
    "width": "377px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
